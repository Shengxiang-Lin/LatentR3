

## Directory Structure

```plaintext
.
|____scripts
|____src
| |____latent
| | (SFT code)
| |____grpo_attention_tuning (RL code)
| | |____latent_grpo_processor.py
| | |____latent_grpo_dataset.py
| | |____model.py
| | |____grpo_trainer.py
| | |____train_noise_grpo.py
| | |____noise_eval.py
| |____utils (others)
|____data (example data and our processing code)
| |____process.py
|____environment.yaml
|____README.md
```

## Environment

```bash
conda env create -f environment.yaml
```

## Train and evaluation

```bash
conda activate trl
# SFT
bash scripts/latent_train.sh
# modified GRPO (change the path to your sft model)
bash scripts/attention_grpo.sh
# for evaluation
bash scripts/grpo_eval.sh
```

> **Note:**
> - We strongly recommend that you use two GPUs to run GRPO. We did not test our code on more GPUs due to computational resources. This is related to the calculation of the advantage function.

## Hyperparameter Search

To achieve expected results, tune the learning rate in GRPO:

- `lr`: {1e-5, 1e-4, 5e-4}

You can tune more hyperparameters to get more impressive results.

> **Note:**
> - Results may vary across different devices even with the same hyperparameters, due to differences in computation precision. We conduct our experiments using 2 NVIDIA A100 GPUs.


CDs_and_Vinyl
0
data/info/CDs_and_Vinyl_5_2015-10-2018-11.txt
6157it [00:00, 323503.70it/s]
[0.00503492 0.00854614 0.00997114 0.01218606 0.01218606]
[0.00503492 0.01120676 0.01477993 0.02160143 0.02160143]

1000
[0.00830078 0.00934127 0.01032854 0.0128167  0.0128167 ]
[0.00830078 0.01025391 0.01269531 0.02050781 0.02050781]

[0.00081208 0.00152555 0.00200807 0.00288363 0.00288363]
[0.00081208 0.00211142 0.00324834 0.00600942 0.00600942]

[0.00990742 0.01150062 0.01256998 0.01386672 0.01386672]
[0.00990742 0.01266851 0.01526718 0.01932759 0.01932759]

[0.01802826 0.02354257 0.02596109 0.02833408 0.02833408]
[0.01802826 0.02761085 0.03345785 0.04076661 0.04076661]

5000
[0.01218126 0.01532512 0.01639448 0.0189625  0.0189625 ]
[0.01218126 0.01770343 0.0203021  0.02842293 0.02842293]

[0.02241351 0.02684315 0.02817807 0.03124412 0.03124412]
[0.02241351 0.03020952 0.03345785 0.04287803 0.04287803]


10000
[0.03037193 0.03556526 0.03772411 0.04057602 0.04057602]
[0.03037193 0.03914244 0.04450219 0.05343511 0.05343511]

[0.02744843 0.032839   0.03469081 0.03741238 0.03741238]
[0.02744843 0.0368686  0.04141627 0.04986195 0.04986195]

all(49251)
[0.04823778 0.05821069 0.06249525 0.06734953 0.06734953]
[0.04823778 0.06529154 0.07568621 0.09046614 0.09046614]
nomul_attn_lr_1e-4
[0.04856261 0.05894542 0.0630972  0.06792573 0.06792573]
[0.04856261 0.06626604 0.07633588 0.0911158  0.0911158 ]

[0.04856261 0.05894542 0.0630972  0.06792573 0.06792573]
[0.04856261 0.06626604 0.07633588 0.0911158  0.0911158 ]

220

[0.0552217  0.06662539 0.07153115 0.07622087 0.07622087]
[0.0552217  0.07471171 0.08673055 0.10118564 0.10118564]

nomul_attn_lr_1e-5
[0.04840019 0.05847558 0.06310277 0.0677338  0.0677338 ]
[0.04840019 0.06561637 0.07682313 0.09095339 0.09095339]
[0.04823778 0.0585007  0.06278526 0.06753585 0.06753585]
[0.04823778 0.06561637 0.07601104 0.09046614 0.09046614]
186
nomul_attn_lr_5e-4
[0.00048725 0.00083335 0.00083335 0.00083335 0.00083335]
[0.00048725 0.00113692 0.00113692 0.00113692 0.00113692]
37706

[0.        0.        0.0001399 0.0001399 0.0001399]
[0.         0.         0.00032483 0.00032483 0.00032483]
36111

nomul_attn_lr_5e-5
[0.05116128 0.06166783 0.0657769  0.07041256 0.07041256]
[0.05116128 0.06902712 0.07909696 0.09338964 0.09338964]
324

[0.05018678 0.06075327 0.06489082 0.06933285 0.06933285]
[0.05018678 0.06821504 0.07828488 0.09192789 0.09192789]
250



nomul_attn_lr_1e-4
[0.03037193 0.03556526 0.03772411 0.04057602 0.04057602]
[0.03037193 0.03914244 0.04450219 0.05343511 0.05343511]
54
nomul_attn_lr_1e-5
[0.03037193 0.03556526 0.03772411 0.04057602 0.04057602]
[0.03037193 0.03914244 0.04450219 0.05343511 0.05343511]
54
nomul_attn_lr_5e-4
[0.03037193 0.03556526 0.03772411 0.04057602 0.04057602]
[0.03037193 0.03914244 0.04450219 0.05343511 0.05343511]
54


Games


10000
[0.01802403 0.02168679 0.02358813 0.02607345 0.02607345]
[0.01802403 0.02463284 0.02930574 0.03698264 0.03698264]
2


data/info/Video_Games_5_2015-10-2018-11.txt
5000it [00:00, 265569.85it/s]
[0.015      0.0190499  0.02037548 0.02227875 0.02227875]
[0.015  0.022  0.0252 0.0312 0.0312]


Group 1 (most popular to least):
  HR@1: 0.0263
  NG@1: 0.0263
  HR@3: 0.0278
  NG@3: 0.0272
  HR@5: 0.0302
  NG@5: 0.0282
  HR@10: 0.0346
  NG@10: 0.0296
Group 2 (most popular to least):
  HR@1: 0.0015
  NG@1: 0.0015
  HR@3: 0.0039
  NG@3: 0.0029
  HR@5: 0.0063
  NG@5: 0.0040
  HR@10: 0.0112
  NG@10: 0.0055
Group 3 (most popular to least):
  HR@1: 0.0019
  NG@1: 0.0019
  HR@3: 0.0063
  NG@3: 0.0044
  HR@5: 0.0093
  NG@5: 0.0056
  HR@10: 0.0122
  NG@10: 0.0065

5000



Group 1 (most popular to least):
  HR@1: 0.0400
  NG@1: 0.0400
  HR@3: 0.0487
  NG@3: 0.0448
  HR@5: 0.0502
  NG@5: 0.0454
  HR@10: 0.0614
  NG@10: 0.0490
Group 2 (most popular to least):
  HR@1: 0.0151
  NG@1: 0.0151
  HR@3: 0.0234
  NG@3: 0.0198
  HR@5: 0.0288
  NG@5: 0.0220
  HR@10: 0.0380
  NG@10: 0.0250
Group 3 (most popular to least):
  HR@1: 0.0122
  NG@1: 0.0122
  HR@3: 0.0185
  NG@3: 0.0159
  HR@5: 0.0214
  NG@5: 0.0171
  HR@10: 0.0292
  NG@10: 0.0197

1e-4 old_group
[0.0552217  0.06662539 0.07153115 0.07622087 0.07622087]
[0.0552217  0.07471171 0.08673055 0.10118564 0.10118564]

Group 1 (most popular to least):
  HR@1: 0.0658
  NG@1: 0.0658
  HR@3: 0.0809
  NG@3: 0.0747
  HR@5: 0.0921
  NG@5: 0.0793
  HR@10: 0.1019
  NG@10: 0.0825
Group 2 (most popular to least):
  HR@1: 0.0507
  NG@1: 0.0507
  HR@3: 0.0736
  NG@3: 0.0640
  HR@5: 0.0838
  NG@5: 0.0682
  HR@10: 0.0994
  NG@10: 0.0732
Group 3 (most popular to least):
  HR@1: 0.0492
  NG@1: 0.0492
  HR@3: 0.0697
  NG@3: 0.0611
  HR@5: 0.0843
  NG@5: 0.0671
  HR@10: 0.1023
  NG@10: 0.0730